## Questions from the discussion with Olga

1. Why do we need optimisable ML pipelines with semantic operators when there agents, automl libraries and pipeline generators?
1. Isn't Copilot already doing a good enough job in generating code for data science tasks?
1. Isn't it too expensive to optimise the operators via cross-validation?
1. Who chooses the exact (physical) operator implementation to execute, the user or the system?
1. Would the user able to specify cost and decide for which parts of pipeline to invest more LLM calls and for which less? Like I know my data is messy, heavily invest into data preparation, but for model do minimal changes