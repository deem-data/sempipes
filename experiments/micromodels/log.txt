--- sempipes.sem_extract_features('['post', 'response']', '{'sp_emotional_reaction_level': "The emotional reaction level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'I'm with you all the way man.' has level 1.\n            A response containing the text 'I agree with him' has level 1.\n            A response containing the text 'Just keep practicing and never give up.' has level 1.\n            A response containing the text 'I'm so sorry you feel that way.' has level 2.\n            A response containing the text 'Holy shit I can relate so well to this.' has level 2.\n            A response containing the text 'really sorry going through this.' has level 2.", 'sp_interpretation_level': "The interpretation level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'People ask me why I'm zoned out most of the time. I'm not zoned out, I'm just in bot mode so I don't have to be attached to anything. It helps me supress the depressive thoughts but every night, it all just flows back in. I hate it.' has level 1.\n            A response containing the text 'i skipped one class today because i don't really get much out of the class, but i forgot that there was 5% of the grade for in-class activities, so i'll probably not skip any more classes' has level 1.\n            A response containing the text 'Actually I find myself taking much longer showers when I'm depressed. Sometimes twice in a day. It's the only time I feel relaxed.' has level 1.\n            A response containing the text 'No, that's what I'm doing and it isn't working.' has level 2.\n            A response containing the text 'I stopped catering to my problems... they just compile. I stopped obsessing over them or handling them in anyway. I have no control over my life. I simply look at my problems from my couch...' has level 2.\n            A response containing the text 'I understand how you feel.' has level 2.        \n            ", 'sp_explorations_level': "The explorations level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'What can we do today that will help?' has level 1.\n            A response containing the text 'What happened?' has level 1.\n            A response containing the text 'What makes you think you're a shitty human being?' has level 1.\n            A response containing the text 'What makes you say these things?' has level 2.\n            A response containing the text 'What do you feel is bringing on said troubling thoughts?' has level 2.\n            A response containing the text 'Do you have any friends that aren't always going to blow sunshine up your ass or a therapist?' has level 2.        \n            "}', '
        Given posts and responses from a social media forum, extract certain linguistic features on a scale from 0.0 to 2.0.        
        ')
	> Generated possible columns: [{'feature_name': 'sp_emotional_reaction_level', 'feature_prompt': "The emotional reaction level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'I'm with you all the way man.' has level 1.\n            A response containing the text 'I agree with him' has level 1.\n            A response containing the text 'Just keep practicing and never give up.' has level 1.\n            A response containing the text 'I'm so sorry you feel that way.' has level 2.\n            A response containing the text 'Holy shit I can relate so well to this.' has level 2.\n            A response containing the text 'really sorry going through this.' has level 2.", 'input_columns': ['post', 'response']}, {'feature_name': 'sp_interpretation_level', 'feature_prompt': "The interpretation level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'People ask me why I'm zoned out most of the time. I'm not zoned out, I'm just in bot mode so I don't have to be attached to anything. It helps me supress the depressive thoughts but every night, it all just flows back in. I hate it.' has level 1.\n            A response containing the text 'i skipped one class today because i don't really get much out of the class, but i forgot that there was 5% of the grade for in-class activities, so i'll probably not skip any more classes' has level 1.\n            A response containing the text 'Actually I find myself taking much longer showers when I'm depressed. Sometimes twice in a day. It's the only time I feel relaxed.' has level 1.\n            A response containing the text 'No, that's what I'm doing and it isn't working.' has level 2.\n            A response containing the text 'I stopped catering to my problems... they just compile. I stopped obsessing over them or handling them in anyway. I have no control over my life. I simply look at my problems from my couch...' has level 2.\n            A response containing the text 'I understand how you feel.' has level 2.        \n            ", 'input_columns': ['post', 'response']}, {'feature_name': 'sp_explorations_level', 'feature_prompt': "The explorations level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'What can we do today that will help?' has level 1.\n            A response containing the text 'What happened?' has level 1.\n            A response containing the text 'What makes you think you're a shitty human being?' has level 1.\n            A response containing the text 'What makes you say these things?' has level 2.\n            A response containing the text 'What do you feel is bringing on said troubling thoughts?' has level 2.\n            A response containing the text 'Do you have any friends that aren't always going to blow sunshine up your ass or a therapist?' has level 2.        \n            ", 'input_columns': ['post', 'response']}]
	> Querying 'openai/gpt-4.1' with 2 messages...'
Chosen GPU: 0
	> An error occurred in attempt 1: Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`
	> Querying 'openai/gpt-4.1' with 4 messages...'
Chosen GPU: 0
Loading checkpoint shards: 100%
 5/5 [00:00<00:00, 52.47it/s]
Device set to use cuda:0
	> An error occurred in attempt 2: name 'end' is not defined
	> Querying 'openai/gpt-4.1' with 6 messages...'
Chosen GPU: 3
Loading checkpoint shards: 100%
 5/5 [00:00<00:00, 76.35it/s]
Device set to use cuda:3
Extracting sp_emotional_reaction_level: 100%|██████████████████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.00s/it]
Extracting sp_interpretation_level:   0%|                                                                                  | 0/3 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors
Extracting sp_interpretation_level: 100%|██████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.12s/it]
Extracting sp_explorations_level: 100%|████████████████████████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.04s/it]
	> Code executed successfully on a sample dataframe.
Chosen GPU: 2
Loading checkpoint shards: 100%
 5/5 [00:00<00:00, 76.79it/s]
Device set to use cuda:2
Extracting sp_emotional_reaction_level: 100%|██████████████████████████████████████████████████████████████████████| 3/3 [00:08<00:00,  2.91s/it]
Extracting sp_interpretation_level:   0%|                                                                                  | 0/3 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors
Extracting sp_interpretation_level: 100%|██████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.08s/it]
Extracting sp_explorations_level: 100%|████████████████████████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.02s/it]
	> Generated columns: ['post', 'response', 'sp_emotional_reaction_level', 'sp_interpretation_level', 'sp_explorations_level']. 
 Code: import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from tqdm import tqdm

def pick_gpu_by_free_mem_torch():
    assert torch.cuda.is_available(), "No CUDA device"
    free = []
    for i in range(torch.cuda.device_count()):
        # returns (free, total) in bytes for that device
        f, t = torch.cuda.mem_get_info(i)
        free.append((f, i))
    free.sort(reverse=True)
    _, idx = free[0]
    print(f"Chosen GPU: {idx}")
    return idx

gpu_idx = pick_gpu_by_free_mem_torch()
device = torch.device(f"cuda:{gpu_idx}")

# Use a strong instruction-following model for zero-shot regression/classification
MODEL_NAME = "google/flan-t5-xxl"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# Remove device_map argument to avoid accelerate dependency
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)
feature_pipe = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    device=gpu_idx,
    batch_size=8,
    max_new_tokens=8,
)

def make_prompt(prompt: str, input_columns: list[str], row: pd.Series) -> str:
    col_text = "\n".join([f"{col}: {row[col]}" for col in input_columns])
    return f"{prompt}\n{col_text}\nAnswer:"

def extract_features(df: pd.DataFrame, features_to_extract: list[dict[str, object]]) -> pd.DataFrame:
    df = df.copy()
    batch_size = 8

    for feat in features_to_extract:
        feature_name = feat['feature_name']
        feature_prompt = feat['feature_prompt']
        input_columns = feat['input_columns']

        prompts = [
            make_prompt(feature_prompt, input_columns, row)
            for _, row in df.iterrows()
        ]

        results = []
        for i in tqdm(range(0, len(prompts), batch_size), desc=f"Extracting {feature_name}"):
            batch_prompts = prompts[i:i+batch_size]
            outputs = feature_pipe(batch_prompts)
            for out in outputs:
                text = out['generated_text']
                import re
                match = re.search(r"([0-2](?:\.\d+)?)", text)
                if match:
                    val = float(match.group(1))
                else:
                    val = None
                results.append(val)
        df[feature_name] = results

    return df

/home/schelter-ldap/.cache/pypoetry/virtualenvs/sempipes-q6T4qXBB-py3.12/lib/python3.12/site-packages/interpret/glassbox/_ebm/_ebm.py:1250: UserWarning: For multiclass we cannot currently visualize pairs and they will be stripped from the global explanations. Set interactions=0 to generate a fully interpretable glassbox model.
  warn(
Processing split 0
--- sempipes.sem_extract_features('['post', 'response']', '{'sp_emotional_reaction_level': "The emotional reaction level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'I'm with you all the way man.' has level 1.\n            A response containing the text 'I agree with him' has level 1.\n            A response containing the text 'Just keep practicing and never give up.' has level 1.\n            A response containing the text 'I'm so sorry you feel that way.' has level 2.\n            A response containing the text 'Holy shit I can relate so well to this.' has level 2.\n            A response containing the text 'really sorry going through this.' has level 2.", 'sp_interpretation_level': "The interpretation level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'People ask me why I'm zoned out most of the time. I'm not zoned out, I'm just in bot mode so I don't have to be attached to anything. It helps me supress the depressive thoughts but every night, it all just flows back in. I hate it.' has level 1.\n            A response containing the text 'i skipped one class today because i don't really get much out of the class, but i forgot that there was 5% of the grade for in-class activities, so i'll probably not skip any more classes' has level 1.\n            A response containing the text 'Actually I find myself taking much longer showers when I'm depressed. Sometimes twice in a day. It's the only time I feel relaxed.' has level 1.\n            A response containing the text 'No, that's what I'm doing and it isn't working.' has level 2.\n            A response containing the text 'I stopped catering to my problems... they just compile. I stopped obsessing over them or handling them in anyway. I have no control over my life. I simply look at my problems from my couch...' has level 2.\n            A response containing the text 'I understand how you feel.' has level 2.        \n            ", 'sp_explorations_level': "The explorations level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'What can we do today that will help?' has level 1.\n            A response containing the text 'What happened?' has level 1.\n            A response containing the text 'What makes you think you're a shitty human being?' has level 1.\n            A response containing the text 'What makes you say these things?' has level 2.\n            A response containing the text 'What do you feel is bringing on said troubling thoughts?' has level 2.\n            A response containing the text 'Do you have any friends that aren't always going to blow sunshine up your ass or a therapist?' has level 2.        \n            "}', '
        Given posts and responses from a social media forum, extract certain linguistic features on a scale from 0.0 to 2.0.        
        ')
	> Generated possible columns: [{'feature_name': 'sp_emotional_reaction_level', 'feature_prompt': "The emotional reaction level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'I'm with you all the way man.' has level 1.\n            A response containing the text 'I agree with him' has level 1.\n            A response containing the text 'Just keep practicing and never give up.' has level 1.\n            A response containing the text 'I'm so sorry you feel that way.' has level 2.\n            A response containing the text 'Holy shit I can relate so well to this.' has level 2.\n            A response containing the text 'really sorry going through this.' has level 2.", 'input_columns': ['post', 'response']}, {'feature_name': 'sp_interpretation_level', 'feature_prompt': "The interpretation level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'People ask me why I'm zoned out most of the time. I'm not zoned out, I'm just in bot mode so I don't have to be attached to anything. It helps me supress the depressive thoughts but every night, it all just flows back in. I hate it.' has level 1.\n            A response containing the text 'i skipped one class today because i don't really get much out of the class, but i forgot that there was 5% of the grade for in-class activities, so i'll probably not skip any more classes' has level 1.\n            A response containing the text 'Actually I find myself taking much longer showers when I'm depressed. Sometimes twice in a day. It's the only time I feel relaxed.' has level 1.\n            A response containing the text 'No, that's what I'm doing and it isn't working.' has level 2.\n            A response containing the text 'I stopped catering to my problems... they just compile. I stopped obsessing over them or handling them in anyway. I have no control over my life. I simply look at my problems from my couch...' has level 2.\n            A response containing the text 'I understand how you feel.' has level 2.        \n            ", 'input_columns': ['post', 'response']}, {'feature_name': 'sp_explorations_level', 'feature_prompt': "The explorations level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'What can we do today that will help?' has level 1.\n            A response containing the text 'What happened?' has level 1.\n            A response containing the text 'What makes you think you're a shitty human being?' has level 1.\n            A response containing the text 'What makes you say these things?' has level 2.\n            A response containing the text 'What do you feel is bringing on said troubling thoughts?' has level 2.\n            A response containing the text 'Do you have any friends that aren't always going to blow sunshine up your ass or a therapist?' has level 2.        \n            ", 'input_columns': ['post', 'response']}]
	> Querying 'openai/gpt-4.1' with 2 messages...'
Chosen GPU: 1
	> An error occurred in attempt 1: Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`
	> Querying 'openai/gpt-4.1' with 4 messages...'
Chosen GPU: 1
	> An error occurred in attempt 2: name 'end' is not defined
	> Querying 'openai/gpt-4.1' with 6 messages...'
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|██████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  3.70it/s]
Extracting sp_interpretation_level: 100%|██████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  3.67it/s]
Extracting sp_explorations_level: 100%|████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  3.99it/s]
	> Code executed successfully on a sample dataframe.
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|██████████████████████████████████████████████████████████████████| 189/189 [00:41<00:00,  4.56it/s]
Extracting sp_interpretation_level: 100%|██████████████████████████████████████████████████████████████████████| 189/189 [00:53<00:00,  3.55it/s]
Extracting sp_explorations_level: 100%|████████████████████████████████████████████████████████████████████████| 189/189 [00:43<00:00,  4.38it/s]
/home/schelter-ldap/.cache/pypoetry/virtualenvs/sempipes-q6T4qXBB-py3.12/lib/python3.12/site-packages/interpret/glassbox/_ebm/_ebm.py:872: UserWarning: Missing values detected. Our visualizations do not currently display missing values. To retain the glassbox nature of the model you need to either set the missing values to an extreme value like -1000 that will be visible on the graphs, or manually examine the missing value score in ebm.term_scores_[term_index][0]
  warn(
	> Generated columns: ['post', 'response', 'sp_emotional_reaction_level', 'sp_interpretation_level', 'sp_explorations_level']. 
 Code: import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm

def pick_gpu_by_free_mem_torch():
    assert torch.cuda.is_available(), "No CUDA device"
    free = []
    for i in range(torch.cuda.device_count()):
        # returns (free, total) in bytes for that device
        f, t = torch.cuda.mem_get_info(i)
        free.append((f, i))
    free.sort(reverse=True)
    _, idx = free[0]
    print(f"Chosen GPU: {idx}")
    return idx

gpu_idx = pick_gpu_by_free_mem_torch()
device = torch.device(f"cuda:{gpu_idx}")

MODEL_NAME = "google/flan-t5-large"  # Use a smaller model for compatibility

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
model = model.to(device)
model.eval()

def batchify(lst, batch_size):
    for i in range(0, len(lst), batch_size):
        yield lst[i:i+batch_size]

def extract_features(df: pd.DataFrame, features_to_extract: list[dict[str, object]]) -> pd.DataFrame:
    BATCH_SIZE = 8

    for feature in features_to_extract:
        feature_name = feature['feature_name']
        feature_prompt = feature['feature_prompt']
        input_columns = feature['input_columns']

        # Prepare input texts for the model
        inputs = []
        for idx, row in df.iterrows():
            text = ""
            for col in input_columns:
                text += f"{col.capitalize()}: {row[col]}\n"
            prompt = f"{feature_prompt}\nGiven the above, output only the numeric value (0, 1, or 2) for the {feature_name.replace('_', ' ')}."
            full_input = f"{text}\n{prompt}"
            inputs.append(full_input)

        outputs = []
        for batch in tqdm(list(batchify(inputs, BATCH_SIZE)), desc=f"Extracting {feature_name}"):
            enc = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=512)
            enc = {k: v.to(device) for k, v in enc.items()}
            with torch.no_grad():
                out = model.generate(
                    **enc,
                    max_new_tokens=4,
                    do_sample=False,
                    num_beams=1,
                )
            decoded = tokenizer.batch_decode(out, skip_special_tokens=True)
            # Post-process: extract the first number (float or int) from the output
            for d in decoded:
                import re
                m = re.search(r"([0-2](?:\.\d+)?)", d)
                if m:
                    val = float(m.group(1))
                else:
                    val = None
                outputs.append(val)

        df[feature_name] = outputs

    return df
/home/schelter-ldap/.cache/pypoetry/virtualenvs/sempipes-q6T4qXBB-py3.12/lib/python3.12/site-packages/interpret/glassbox/_ebm/_ebm.py:1250: UserWarning: For multiclass we cannot currently visualize pairs and they will be stripped from the global explanations. Set interactions=0 to generate a fully interpretable glassbox model.
  warn(
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|██████████████████████████████████████████████████████████████████| 189/189 [00:41<00:00,  4.51it/s]
Extracting sp_interpretation_level: 100%|██████████████████████████████████████████████████████████████████████| 189/189 [00:53<00:00,  3.53it/s]
Extracting sp_explorations_level: 100%|████████████████████████████████████████████████████████████████████████| 189/189 [00:43<00:00,  4.36it/s]
	> Generated columns: ['post', 'response', 'sp_emotional_reaction_level', 'sp_interpretation_level', 'sp_explorations_level']. 
 Code: import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm

def pick_gpu_by_free_mem_torch():
    assert torch.cuda.is_available(), "No CUDA device"
    free = []
    for i in range(torch.cuda.device_count()):
        # returns (free, total) in bytes for that device
        f, t = torch.cuda.mem_get_info(i)
        free.append((f, i))
    free.sort(reverse=True)
    _, idx = free[0]
    print(f"Chosen GPU: {idx}")
    return idx

gpu_idx = pick_gpu_by_free_mem_torch()
device = torch.device(f"cuda:{gpu_idx}")

MODEL_NAME = "google/flan-t5-large"  # Use a smaller model for compatibility

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
model = model.to(device)
model.eval()

def batchify(lst, batch_size):
    for i in range(0, len(lst), batch_size):
        yield lst[i:i+batch_size]

def extract_features(df: pd.DataFrame, features_to_extract: list[dict[str, object]]) -> pd.DataFrame:
    BATCH_SIZE = 8

    for feature in features_to_extract:
        feature_name = feature['feature_name']
        feature_prompt = feature['feature_prompt']
        input_columns = feature['input_columns']

        # Prepare input texts for the model
        inputs = []
        for idx, row in df.iterrows():
            text = ""
            for col in input_columns:
                text += f"{col.capitalize()}: {row[col]}\n"
            prompt = f"{feature_prompt}\nGiven the above, output only the numeric value (0, 1, or 2) for the {feature_name.replace('_', ' ')}."
            full_input = f"{text}\n{prompt}"
            inputs.append(full_input)

        outputs = []
        for batch in tqdm(list(batchify(inputs, BATCH_SIZE)), desc=f"Extracting {feature_name}"):
            enc = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=512)
            enc = {k: v.to(device) for k, v in enc.items()}
            with torch.no_grad():
                out = model.generate(
                    **enc,
                    max_new_tokens=4,
                    do_sample=False,
                    num_beams=1,
                )
            decoded = tokenizer.batch_decode(out, skip_special_tokens=True)
            # Post-process: extract the first number (float or int) from the output
            for d in decoded:
                import re
                m = re.search(r"([0-2](?:\.\d+)?)", d)
                if m:
                    val = float(m.group(1))
                else:
                    val = None
                outputs.append(val)

        df[feature_name] = outputs

    return df
F1 score on 0: 0.7063492063492064
Processing split 1
--- sempipes.sem_extract_features('['post', 'response']', '{'sp_emotional_reaction_level': "The emotional reaction level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'I'm with you all the way man.' has level 1.\n            A response containing the text 'I agree with him' has level 1.\n            A response containing the text 'Just keep practicing and never give up.' has level 1.\n            A response containing the text 'I'm so sorry you feel that way.' has level 2.\n            A response containing the text 'Holy shit I can relate so well to this.' has level 2.\n            A response containing the text 'really sorry going through this.' has level 2.", 'sp_interpretation_level': "The interpretation level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'People ask me why I'm zoned out most of the time. I'm not zoned out, I'm just in bot mode so I don't have to be attached to anything. It helps me supress the depressive thoughts but every night, it all just flows back in. I hate it.' has level 1.\n            A response containing the text 'i skipped one class today because i don't really get much out of the class, but i forgot that there was 5% of the grade for in-class activities, so i'll probably not skip any more classes' has level 1.\n            A response containing the text 'Actually I find myself taking much longer showers when I'm depressed. Sometimes twice in a day. It's the only time I feel relaxed.' has level 1.\n            A response containing the text 'No, that's what I'm doing and it isn't working.' has level 2.\n            A response containing the text 'I stopped catering to my problems... they just compile. I stopped obsessing over them or handling them in anyway. I have no control over my life. I simply look at my problems from my couch...' has level 2.\n            A response containing the text 'I understand how you feel.' has level 2.        \n            ", 'sp_explorations_level': "The explorations level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'What can we do today that will help?' has level 1.\n            A response containing the text 'What happened?' has level 1.\n            A response containing the text 'What makes you think you're a shitty human being?' has level 1.\n            A response containing the text 'What makes you say these things?' has level 2.\n            A response containing the text 'What do you feel is bringing on said troubling thoughts?' has level 2.\n            A response containing the text 'Do you have any friends that aren't always going to blow sunshine up your ass or a therapist?' has level 2.        \n            "}', '
        Given posts and responses from a social media forum, extract certain linguistic features on a scale from 0.0 to 2.0.        
        ')
	> Generated possible columns: [{'feature_name': 'sp_emotional_reaction_level', 'feature_prompt': "The emotional reaction level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'I'm with you all the way man.' has level 1.\n            A response containing the text 'I agree with him' has level 1.\n            A response containing the text 'Just keep practicing and never give up.' has level 1.\n            A response containing the text 'I'm so sorry you feel that way.' has level 2.\n            A response containing the text 'Holy shit I can relate so well to this.' has level 2.\n            A response containing the text 'really sorry going through this.' has level 2.", 'input_columns': ['post', 'response']}, {'feature_name': 'sp_interpretation_level', 'feature_prompt': "The interpretation level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'People ask me why I'm zoned out most of the time. I'm not zoned out, I'm just in bot mode so I don't have to be attached to anything. It helps me supress the depressive thoughts but every night, it all just flows back in. I hate it.' has level 1.\n            A response containing the text 'i skipped one class today because i don't really get much out of the class, but i forgot that there was 5% of the grade for in-class activities, so i'll probably not skip any more classes' has level 1.\n            A response containing the text 'Actually I find myself taking much longer showers when I'm depressed. Sometimes twice in a day. It's the only time I feel relaxed.' has level 1.\n            A response containing the text 'No, that's what I'm doing and it isn't working.' has level 2.\n            A response containing the text 'I stopped catering to my problems... they just compile. I stopped obsessing over them or handling them in anyway. I have no control over my life. I simply look at my problems from my couch...' has level 2.\n            A response containing the text 'I understand how you feel.' has level 2.        \n            ", 'input_columns': ['post', 'response']}, {'feature_name': 'sp_explorations_level', 'feature_prompt': "The explorations level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'What can we do today that will help?' has level 1.\n            A response containing the text 'What happened?' has level 1.\n            A response containing the text 'What makes you think you're a shitty human being?' has level 1.\n            A response containing the text 'What makes you say these things?' has level 2.\n            A response containing the text 'What do you feel is bringing on said troubling thoughts?' has level 2.\n            A response containing the text 'Do you have any friends that aren't always going to blow sunshine up your ass or a therapist?' has level 2.        \n            ", 'input_columns': ['post', 'response']}]
	> Querying 'openai/gpt-4.1' with 2 messages...'
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.73it/s]
Extracting sp_interpretation_level: 100%|██████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.37it/s]
Extracting sp_explorations_level: 100%|████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.89it/s]
	> Code executed successfully on a sample dataframe.
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|████████████████████████████████████████████████████████████████████| 95/95 [00:41<00:00,  2.27it/s]
Extracting sp_interpretation_level: 100%|████████████████████████████████████████████████████████████████████████| 95/95 [00:48<00:00,  1.98it/s]
Extracting sp_explorations_level: 100%|██████████████████████████████████████████████████████████████████████████| 95/95 [00:42<00:00,  2.25it/s]
	> Generated columns: ['post', 'response', 'sp_emotional_reaction_level', 'sp_interpretation_level', 'sp_explorations_level']. 
 Code: import torch
import pandas as pd
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    pipeline,
)
from tqdm import tqdm

def pick_gpu_by_free_mem_torch():
    assert torch.cuda.is_available(), "No CUDA device"
    free = []
    for i in range(torch.cuda.device_count()):
        # returns (free, total) in bytes for that device
        f, t = torch.cuda.mem_get_info(i)
        free.append((f, i))
    free.sort(reverse=True)
    _, idx = free[0]
    print(f"Chosen GPU: {idx}")
    return idx

gpu_idx = pick_gpu_by_free_mem_torch()
device = torch.device(f"cuda:{gpu_idx}") # Use the selected GPU for model inference

# You can change the model to a more powerful one if needed
GEN_MODEL_NAME = "google/flan-t5-large"

def extract_features(df: pd.DataFrame, features_to_extract: list[dict[str, object]]) -> pd.DataFrame:
    # Load model and tokenizer once
    tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL_NAME)
    model = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL_NAME).to(device)
    model.eval()

    # Set batch size for GPU efficiency
    BATCH_SIZE = 16

    for feature in features_to_extract:
        feature_name = feature['feature_name']
        feature_prompt = feature['feature_prompt']
        input_columns = feature['input_columns']

        # Prepare prompts for all rows
        prompts = []
        for idx, row in df.iterrows():
            # Concatenate input columns with clear separation
            input_text = ""
            for col in input_columns:
                input_text += f"{col}: {row[col]}\n"
            # Compose the prompt
            prompt = (
                f"{feature_prompt}\n"
                f"Given the following input, output only the level as a number (0, 1, or 2):\n"
                f"{input_text}"
                f"Level:"
            )
            prompts.append(prompt)

        # Batch inference
        results = []
        for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=f"Extracting {feature_name}"):
            batch_prompts = prompts[i:i+BATCH_SIZE]
            inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=4,
                    do_sample=False,
                    num_beams=1,
                )
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            # Post-process: extract the first number (0, 1, or 2) from the output
            for out in decoded:
                # Try to extract a float or int from the output
                val = None
                for token in out.split():
                    try:
                        val = float(token)
                        break
                    except ValueError:
                        continue
                if val is None:
                    val = 0.0  # fallback
                results.append(val)

        # Add the results as a new column
        df[feature_name] = results

    return df
/home/schelter-ldap/.cache/pypoetry/virtualenvs/sempipes-q6T4qXBB-py3.12/lib/python3.12/site-packages/interpret/glassbox/_ebm/_ebm.py:1250: UserWarning: For multiclass we cannot currently visualize pairs and they will be stripped from the global explanations. Set interactions=0 to generate a fully interpretable glassbox model.
  warn(
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|████████████████████████████████████████████████████████████████████| 95/95 [00:40<00:00,  2.34it/s]
Extracting sp_interpretation_level: 100%|████████████████████████████████████████████████████████████████████████| 95/95 [00:47<00:00,  1.98it/s]
Extracting sp_explorations_level: 100%|██████████████████████████████████████████████████████████████████████████| 95/95 [00:41<00:00,  2.31it/s]
	> Generated columns: ['post', 'response', 'sp_emotional_reaction_level', 'sp_interpretation_level', 'sp_explorations_level']. 
 Code: import torch
import pandas as pd
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    pipeline,
)
from tqdm import tqdm

def pick_gpu_by_free_mem_torch():
    assert torch.cuda.is_available(), "No CUDA device"
    free = []
    for i in range(torch.cuda.device_count()):
        # returns (free, total) in bytes for that device
        f, t = torch.cuda.mem_get_info(i)
        free.append((f, i))
    free.sort(reverse=True)
    _, idx = free[0]
    print(f"Chosen GPU: {idx}")
    return idx

gpu_idx = pick_gpu_by_free_mem_torch()
device = torch.device(f"cuda:{gpu_idx}") # Use the selected GPU for model inference

# You can change the model to a more powerful one if needed
GEN_MODEL_NAME = "google/flan-t5-large"

def extract_features(df: pd.DataFrame, features_to_extract: list[dict[str, object]]) -> pd.DataFrame:
    # Load model and tokenizer once
    tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL_NAME)
    model = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL_NAME).to(device)
    model.eval()

    # Set batch size for GPU efficiency
    BATCH_SIZE = 16

    for feature in features_to_extract:
        feature_name = feature['feature_name']
        feature_prompt = feature['feature_prompt']
        input_columns = feature['input_columns']

        # Prepare prompts for all rows
        prompts = []
        for idx, row in df.iterrows():
            # Concatenate input columns with clear separation
            input_text = ""
            for col in input_columns:
                input_text += f"{col}: {row[col]}\n"
            # Compose the prompt
            prompt = (
                f"{feature_prompt}\n"
                f"Given the following input, output only the level as a number (0, 1, or 2):\n"
                f"{input_text}"
                f"Level:"
            )
            prompts.append(prompt)

        # Batch inference
        results = []
        for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=f"Extracting {feature_name}"):
            batch_prompts = prompts[i:i+BATCH_SIZE]
            inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=4,
                    do_sample=False,
                    num_beams=1,
                )
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            # Post-process: extract the first number (0, 1, or 2) from the output
            for out in decoded:
                # Try to extract a float or int from the output
                val = None
                for token in out.split():
                    try:
                        val = float(token)
                        break
                    except ValueError:
                        continue
                if val is None:
                    val = 0.0  # fallback
                results.append(val)

        # Add the results as a new column
        df[feature_name] = results

    return df
F1 score on 1: 0.6931216931216931
Processing split 2
--- sempipes.sem_extract_features('['post', 'response']', '{'sp_emotional_reaction_level': "The emotional reaction level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'I'm with you all the way man.' has level 1.\n            A response containing the text 'I agree with him' has level 1.\n            A response containing the text 'Just keep practicing and never give up.' has level 1.\n            A response containing the text 'I'm so sorry you feel that way.' has level 2.\n            A response containing the text 'Holy shit I can relate so well to this.' has level 2.\n            A response containing the text 'really sorry going through this.' has level 2.", 'sp_interpretation_level': "The interpretation level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'People ask me why I'm zoned out most of the time. I'm not zoned out, I'm just in bot mode so I don't have to be attached to anything. It helps me supress the depressive thoughts but every night, it all just flows back in. I hate it.' has level 1.\n            A response containing the text 'i skipped one class today because i don't really get much out of the class, but i forgot that there was 5% of the grade for in-class activities, so i'll probably not skip any more classes' has level 1.\n            A response containing the text 'Actually I find myself taking much longer showers when I'm depressed. Sometimes twice in a day. It's the only time I feel relaxed.' has level 1.\n            A response containing the text 'No, that's what I'm doing and it isn't working.' has level 2.\n            A response containing the text 'I stopped catering to my problems... they just compile. I stopped obsessing over them or handling them in anyway. I have no control over my life. I simply look at my problems from my couch...' has level 2.\n            A response containing the text 'I understand how you feel.' has level 2.        \n            ", 'sp_explorations_level': "The explorations level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'What can we do today that will help?' has level 1.\n            A response containing the text 'What happened?' has level 1.\n            A response containing the text 'What makes you think you're a shitty human being?' has level 1.\n            A response containing the text 'What makes you say these things?' has level 2.\n            A response containing the text 'What do you feel is bringing on said troubling thoughts?' has level 2.\n            A response containing the text 'Do you have any friends that aren't always going to blow sunshine up your ass or a therapist?' has level 2.        \n            "}', '
        Given posts and responses from a social media forum, extract certain linguistic features on a scale from 0.0 to 2.0.        
        ')
	> Generated possible columns: [{'feature_name': 'sp_emotional_reaction_level', 'feature_prompt': "The emotional reaction level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'I'm with you all the way man.' has level 1.\n            A response containing the text 'I agree with him' has level 1.\n            A response containing the text 'Just keep practicing and never give up.' has level 1.\n            A response containing the text 'I'm so sorry you feel that way.' has level 2.\n            A response containing the text 'Holy shit I can relate so well to this.' has level 2.\n            A response containing the text 'really sorry going through this.' has level 2.", 'input_columns': ['post', 'response']}, {'feature_name': 'sp_interpretation_level', 'feature_prompt': "The interpretation level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'People ask me why I'm zoned out most of the time. I'm not zoned out, I'm just in bot mode so I don't have to be attached to anything. It helps me supress the depressive thoughts but every night, it all just flows back in. I hate it.' has level 1.\n            A response containing the text 'i skipped one class today because i don't really get much out of the class, but i forgot that there was 5% of the grade for in-class activities, so i'll probably not skip any more classes' has level 1.\n            A response containing the text 'Actually I find myself taking much longer showers when I'm depressed. Sometimes twice in a day. It's the only time I feel relaxed.' has level 1.\n            A response containing the text 'No, that's what I'm doing and it isn't working.' has level 2.\n            A response containing the text 'I stopped catering to my problems... they just compile. I stopped obsessing over them or handling them in anyway. I have no control over my life. I simply look at my problems from my couch...' has level 2.\n            A response containing the text 'I understand how you feel.' has level 2.        \n            ", 'input_columns': ['post', 'response']}, {'feature_name': 'sp_explorations_level', 'feature_prompt': "The explorations level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'What can we do today that will help?' has level 1.\n            A response containing the text 'What happened?' has level 1.\n            A response containing the text 'What makes you think you're a shitty human being?' has level 1.\n            A response containing the text 'What makes you say these things?' has level 2.\n            A response containing the text 'What do you feel is bringing on said troubling thoughts?' has level 2.\n            A response containing the text 'Do you have any friends that aren't always going to blow sunshine up your ass or a therapist?' has level 2.        \n            ", 'input_columns': ['post', 'response']}]
	> Querying 'openai/gpt-4.1' with 2 messages...'
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.03it/s]
Extracting sp_interpretation_level: 100%|██████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.20it/s]
Extracting sp_explorations_level: 100%|████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.44it/s]
	> Code executed successfully on a sample dataframe.
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|████████████████████████████████████████████████████████████████████| 95/95 [00:42<00:00,  2.24it/s]
Extracting sp_interpretation_level: 100%|████████████████████████████████████████████████████████████████████████| 95/95 [00:48<00:00,  1.94it/s]
Extracting sp_explorations_level: 100%|██████████████████████████████████████████████████████████████████████████| 95/95 [00:43<00:00,  2.17it/s]
	> Generated columns: ['post', 'response', 'sp_emotional_reaction_level', 'sp_interpretation_level', 'sp_explorations_level']. 
 Code: import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from tqdm import tqdm

def pick_gpu_by_free_mem_torch():
    assert torch.cuda.is_available(), "No CUDA device"
    free = []
    for i in range(torch.cuda.device_count()):
        # returns (free, total) in bytes for that device
        f, t = torch.cuda.mem_get_info(i)
        free.append((f, i))
    free.sort(reverse=True)
    _, idx = free[0]
    print(f"Chosen GPU: {idx}")
    return idx

gpu_idx = pick_gpu_by_free_mem_torch()
device = torch.device(f"cuda:{gpu_idx}")

# We'll use a strong instruction-following model for zero-shot regression/classification
# E.g., 'google/flan-t5-large' or 'google/flan-t5-xl' (if enough GPU memory)
MODEL_NAME = "google/flan-t5-large"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)

def batchify(lst, batch_size):
    for i in range(0, len(lst), batch_size):
        yield lst[i:i+batch_size]

def extract_features(df: pd.DataFrame, features_to_extract: list[dict[str, object]]) -> pd.DataFrame:
    BATCH_SIZE = 16  # Adjust based on GPU memory

    for feature in features_to_extract:
        feature_name = feature['feature_name']
        feature_prompt = feature['feature_prompt']
        input_columns = feature['input_columns']

        # Prepare inputs: concatenate columns as context for the prompt
        inputs = []
        for idx, row in df.iterrows():
            # You can adjust the input format as needed
            context = ""
            for col in input_columns:
                context += f"{col.capitalize()}: {row[col]}\n"
            # Compose the prompt for the model
            full_prompt = (
                f"{feature_prompt}\n"
                f"Given the following:\n"
                f"{context}"
                f"Return only the numeric value (0, 1, or 2) as a float."
            )
            inputs.append(full_prompt)

        results = []
        for batch in tqdm(list(batchify(inputs, BATCH_SIZE)), desc=f"Extracting {feature_name}"):
            # Tokenize
            enc = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=512)
            enc = {k: v.to(device) for k, v in enc.items()}
            # Generate
            with torch.no_grad():
                outputs = model.generate(
                    **enc,
                    max_new_tokens=8,
                    do_sample=False,
                    num_beams=1,
                )
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            # Post-process: extract the first float in the output
            for out in decoded:
                try:
                    # Find the first float in the output
                    import re
                    match = re.search(r"([0-2](?:\.\d+)?)", out)
                    if match:
                        val = float(match.group(1))
                    else:
                        val = None
                except Exception:
                    val = None
                results.append(val)

        # Add to DataFrame
        df[feature_name] = results

    return df

/home/schelter-ldap/.cache/pypoetry/virtualenvs/sempipes-q6T4qXBB-py3.12/lib/python3.12/site-packages/interpret/glassbox/_ebm/_ebm.py:872: UserWarning: Missing values detected. Our visualizations do not currently display missing values. To retain the glassbox nature of the model you need to either set the missing values to an extreme value like -1000 that will be visible on the graphs, or manually examine the missing value score in ebm.term_scores_[term_index][0]
  warn(
/home/schelter-ldap/.cache/pypoetry/virtualenvs/sempipes-q6T4qXBB-py3.12/lib/python3.12/site-packages/interpret/glassbox/_ebm/_ebm.py:1250: UserWarning: For multiclass we cannot currently visualize pairs and they will be stripped from the global explanations. Set interactions=0 to generate a fully interpretable glassbox model.
  warn(
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|████████████████████████████████████████████████████████████████████| 95/95 [00:42<00:00,  2.23it/s]
Extracting sp_interpretation_level: 100%|████████████████████████████████████████████████████████████████████████| 95/95 [00:48<00:00,  1.96it/s]
Extracting sp_explorations_level: 100%|██████████████████████████████████████████████████████████████████████████| 95/95 [00:43<00:00,  2.17it/s]
	> Generated columns: ['post', 'response', 'sp_emotional_reaction_level', 'sp_interpretation_level', 'sp_explorations_level']. 
 Code: import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from tqdm import tqdm

def pick_gpu_by_free_mem_torch():
    assert torch.cuda.is_available(), "No CUDA device"
    free = []
    for i in range(torch.cuda.device_count()):
        # returns (free, total) in bytes for that device
        f, t = torch.cuda.mem_get_info(i)
        free.append((f, i))
    free.sort(reverse=True)
    _, idx = free[0]
    print(f"Chosen GPU: {idx}")
    return idx

gpu_idx = pick_gpu_by_free_mem_torch()
device = torch.device(f"cuda:{gpu_idx}")

# We'll use a strong instruction-following model for zero-shot regression/classification
# E.g., 'google/flan-t5-large' or 'google/flan-t5-xl' (if enough GPU memory)
MODEL_NAME = "google/flan-t5-large"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)

def batchify(lst, batch_size):
    for i in range(0, len(lst), batch_size):
        yield lst[i:i+batch_size]

def extract_features(df: pd.DataFrame, features_to_extract: list[dict[str, object]]) -> pd.DataFrame:
    BATCH_SIZE = 16  # Adjust based on GPU memory

    for feature in features_to_extract:
        feature_name = feature['feature_name']
        feature_prompt = feature['feature_prompt']
        input_columns = feature['input_columns']

        # Prepare inputs: concatenate columns as context for the prompt
        inputs = []
        for idx, row in df.iterrows():
            # You can adjust the input format as needed
            context = ""
            for col in input_columns:
                context += f"{col.capitalize()}: {row[col]}\n"
            # Compose the prompt for the model
            full_prompt = (
                f"{feature_prompt}\n"
                f"Given the following:\n"
                f"{context}"
                f"Return only the numeric value (0, 1, or 2) as a float."
            )
            inputs.append(full_prompt)

        results = []
        for batch in tqdm(list(batchify(inputs, BATCH_SIZE)), desc=f"Extracting {feature_name}"):
            # Tokenize
            enc = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=512)
            enc = {k: v.to(device) for k, v in enc.items()}
            # Generate
            with torch.no_grad():
                outputs = model.generate(
                    **enc,
                    max_new_tokens=8,
                    do_sample=False,
                    num_beams=1,
                )
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            # Post-process: extract the first float in the output
            for out in decoded:
                try:
                    # Find the first float in the output
                    import re
                    match = re.search(r"([0-2](?:\.\d+)?)", out)
                    if match:
                        val = float(match.group(1))
                    else:
                        val = None
                except Exception:
                    val = None
                results.append(val)

        # Add to DataFrame
        df[feature_name] = results

    return df
F1 score on 2: 0.66005291005291
Processing split 3
--- sempipes.sem_extract_features('['post', 'response']', '{'sp_emotional_reaction_level': "The emotional reaction level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'I'm with you all the way man.' has level 1.\n            A response containing the text 'I agree with him' has level 1.\n            A response containing the text 'Just keep practicing and never give up.' has level 1.\n            A response containing the text 'I'm so sorry you feel that way.' has level 2.\n            A response containing the text 'Holy shit I can relate so well to this.' has level 2.\n            A response containing the text 'really sorry going through this.' has level 2.", 'sp_interpretation_level': "The interpretation level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'People ask me why I'm zoned out most of the time. I'm not zoned out, I'm just in bot mode so I don't have to be attached to anything. It helps me supress the depressive thoughts but every night, it all just flows back in. I hate it.' has level 1.\n            A response containing the text 'i skipped one class today because i don't really get much out of the class, but i forgot that there was 5% of the grade for in-class activities, so i'll probably not skip any more classes' has level 1.\n            A response containing the text 'Actually I find myself taking much longer showers when I'm depressed. Sometimes twice in a day. It's the only time I feel relaxed.' has level 1.\n            A response containing the text 'No, that's what I'm doing and it isn't working.' has level 2.\n            A response containing the text 'I stopped catering to my problems... they just compile. I stopped obsessing over them or handling them in anyway. I have no control over my life. I simply look at my problems from my couch...' has level 2.\n            A response containing the text 'I understand how you feel.' has level 2.        \n            ", 'sp_explorations_level': "The explorations level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'What can we do today that will help?' has level 1.\n            A response containing the text 'What happened?' has level 1.\n            A response containing the text 'What makes you think you're a shitty human being?' has level 1.\n            A response containing the text 'What makes you say these things?' has level 2.\n            A response containing the text 'What do you feel is bringing on said troubling thoughts?' has level 2.\n            A response containing the text 'Do you have any friends that aren't always going to blow sunshine up your ass or a therapist?' has level 2.        \n            "}', '
        Given posts and responses from a social media forum, extract certain linguistic features on a scale from 0.0 to 2.0.        
        ')
	> Generated possible columns: [{'feature_name': 'sp_emotional_reaction_level', 'feature_prompt': "The emotional reaction level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'I'm with you all the way man.' has level 1.\n            A response containing the text 'I agree with him' has level 1.\n            A response containing the text 'Just keep practicing and never give up.' has level 1.\n            A response containing the text 'I'm so sorry you feel that way.' has level 2.\n            A response containing the text 'Holy shit I can relate so well to this.' has level 2.\n            A response containing the text 'really sorry going through this.' has level 2.", 'input_columns': ['post', 'response']}, {'feature_name': 'sp_interpretation_level', 'feature_prompt': "The interpretation level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'People ask me why I'm zoned out most of the time. I'm not zoned out, I'm just in bot mode so I don't have to be attached to anything. It helps me supress the depressive thoughts but every night, it all just flows back in. I hate it.' has level 1.\n            A response containing the text 'i skipped one class today because i don't really get much out of the class, but i forgot that there was 5% of the grade for in-class activities, so i'll probably not skip any more classes' has level 1.\n            A response containing the text 'Actually I find myself taking much longer showers when I'm depressed. Sometimes twice in a day. It's the only time I feel relaxed.' has level 1.\n            A response containing the text 'No, that's what I'm doing and it isn't working.' has level 2.\n            A response containing the text 'I stopped catering to my problems... they just compile. I stopped obsessing over them or handling them in anyway. I have no control over my life. I simply look at my problems from my couch...' has level 2.\n            A response containing the text 'I understand how you feel.' has level 2.        \n            ", 'input_columns': ['post', 'response']}, {'feature_name': 'sp_explorations_level', 'feature_prompt': "The explorations level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'What can we do today that will help?' has level 1.\n            A response containing the text 'What happened?' has level 1.\n            A response containing the text 'What makes you think you're a shitty human being?' has level 1.\n            A response containing the text 'What makes you say these things?' has level 2.\n            A response containing the text 'What do you feel is bringing on said troubling thoughts?' has level 2.\n            A response containing the text 'Do you have any friends that aren't always going to blow sunshine up your ass or a therapist?' has level 2.        \n            ", 'input_columns': ['post', 'response']}]
	> Querying 'openai/gpt-4.1' with 2 messages...'
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.45it/s]
Extracting sp_interpretation_level: 100%|██████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.38it/s]
Extracting sp_explorations_level: 100%|████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.65it/s]
	> Code executed successfully on a sample dataframe.
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|████████████████████████████████████████████████████████████████████| 95/95 [00:43<00:00,  2.19it/s]
Extracting sp_interpretation_level: 100%|████████████████████████████████████████████████████████████████████████| 95/95 [00:48<00:00,  1.95it/s]
Extracting sp_explorations_level: 100%|██████████████████████████████████████████████████████████████████████████| 95/95 [00:44<00:00,  2.14it/s]
	> Generated columns: ['post', 'response', 'sp_emotional_reaction_level', 'sp_interpretation_level', 'sp_explorations_level']. 
 Code: import torch
import pandas as pd
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    pipeline,
)
from tqdm import tqdm

def pick_gpu_by_free_mem_torch():
    assert torch.cuda.is_available(), "No CUDA device"
    free = []
    for i in range(torch.cuda.device_count()):
        # returns (free, total) in bytes for that device
        f, t = torch.cuda.mem_get_info(i)
        free.append((f, i))
    free.sort(reverse=True)
    _, idx = free[0]
    print(f"Chosen GPU: {idx}")
    return idx

gpu_idx = pick_gpu_by_free_mem_torch()
device = torch.device(f"cuda:{gpu_idx}") # Use the selected GPU for model inference

# You can adjust the model to a more recent or more powerful one if needed
GEN_MODEL_NAME = "google/flan-t5-large"

def extract_features(df: pd.DataFrame, features_to_extract: list[dict[str, object]]) -> pd.DataFrame:
    # Load model and tokenizer once
    tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL_NAME)
    model = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL_NAME).to(device)
    model.eval()

    # Set batch size for GPU efficiency
    BATCH_SIZE = 16

    for feature in features_to_extract:
        feature_name = feature['feature_name']
        feature_prompt = feature['feature_prompt']
        input_columns = feature['input_columns']

        # Prepare prompts for all rows
        prompts = []
        for idx, row in df.iterrows():
            # Concatenate input columns with clear separation
            input_text = " ".join([f"{col}: {str(row[col])}" for col in input_columns])
            prompt = (
                f"{feature_prompt}\n"
                f"Given the following post and response, what is the {feature_name.replace('_', ' ')}? "
                f"Return only a number (0, 1, or 2).\n"
                f"{input_text}"
            )
            prompts.append(prompt)

        # Store results here
        feature_values = []

        # Batched inference with tqdm
        for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=f"Extracting {feature_name}"):
            batch_prompts = prompts[i:i+BATCH_SIZE]
            # Tokenize
            inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True, truncation=True, max_length=512)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=4,
                    do_sample=False,
                    num_beams=1,
                )
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            # Post-process: extract the first number (0, 1, or 2) or float in the output
            for out in decoded:
                # Try to extract a float or int from the output
                import re
                match = re.search(r"([0-2](?:\.\d+)?)", out)
                if match:
                    val = float(match.group(1))
                else:
                    val = None
                feature_values.append(val)

        # Add to DataFrame
        df[feature_name] = feature_values

    return df

/home/schelter-ldap/.cache/pypoetry/virtualenvs/sempipes-q6T4qXBB-py3.12/lib/python3.12/site-packages/interpret/glassbox/_ebm/_ebm.py:1250: UserWarning: For multiclass we cannot currently visualize pairs and they will be stripped from the global explanations. Set interactions=0 to generate a fully interpretable glassbox model.
  warn(
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|████████████████████████████████████████████████████████████████████| 95/95 [00:42<00:00,  2.21it/s]
Extracting sp_interpretation_level: 100%|████████████████████████████████████████████████████████████████████████| 95/95 [00:48<00:00,  1.96it/s]
Extracting sp_explorations_level: 100%|██████████████████████████████████████████████████████████████████████████| 95/95 [00:43<00:00,  2.17it/s]
	> Generated columns: ['post', 'response', 'sp_emotional_reaction_level', 'sp_interpretation_level', 'sp_explorations_level']. 
 Code: import torch
import pandas as pd
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    pipeline,
)
from tqdm import tqdm

def pick_gpu_by_free_mem_torch():
    assert torch.cuda.is_available(), "No CUDA device"
    free = []
    for i in range(torch.cuda.device_count()):
        # returns (free, total) in bytes for that device
        f, t = torch.cuda.mem_get_info(i)
        free.append((f, i))
    free.sort(reverse=True)
    _, idx = free[0]
    print(f"Chosen GPU: {idx}")
    return idx

gpu_idx = pick_gpu_by_free_mem_torch()
device = torch.device(f"cuda:{gpu_idx}") # Use the selected GPU for model inference

# You can adjust the model to a more recent or more powerful one if needed
GEN_MODEL_NAME = "google/flan-t5-large"

def extract_features(df: pd.DataFrame, features_to_extract: list[dict[str, object]]) -> pd.DataFrame:
    # Load model and tokenizer once
    tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL_NAME)
    model = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL_NAME).to(device)
    model.eval()

    # Set batch size for GPU efficiency
    BATCH_SIZE = 16

    for feature in features_to_extract:
        feature_name = feature['feature_name']
        feature_prompt = feature['feature_prompt']
        input_columns = feature['input_columns']

        # Prepare prompts for all rows
        prompts = []
        for idx, row in df.iterrows():
            # Concatenate input columns with clear separation
            input_text = " ".join([f"{col}: {str(row[col])}" for col in input_columns])
            prompt = (
                f"{feature_prompt}\n"
                f"Given the following post and response, what is the {feature_name.replace('_', ' ')}? "
                f"Return only a number (0, 1, or 2).\n"
                f"{input_text}"
            )
            prompts.append(prompt)

        # Store results here
        feature_values = []

        # Batched inference with tqdm
        for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=f"Extracting {feature_name}"):
            batch_prompts = prompts[i:i+BATCH_SIZE]
            # Tokenize
            inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True, truncation=True, max_length=512)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=4,
                    do_sample=False,
                    num_beams=1,
                )
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            # Post-process: extract the first number (0, 1, or 2) or float in the output
            for out in decoded:
                # Try to extract a float or int from the output
                import re
                match = re.search(r"([0-2](?:\.\d+)?)", out)
                if match:
                    val = float(match.group(1))
                else:
                    val = None
                feature_values.append(val)

        # Add to DataFrame
        df[feature_name] = feature_values

    return df
F1 score on 3: 0.66005291005291
Processing split 4
--- sempipes.sem_extract_features('['post', 'response']', '{'sp_emotional_reaction_level': "The emotional reaction level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'I'm with you all the way man.' has level 1.\n            A response containing the text 'I agree with him' has level 1.\n            A response containing the text 'Just keep practicing and never give up.' has level 1.\n            A response containing the text 'I'm so sorry you feel that way.' has level 2.\n            A response containing the text 'Holy shit I can relate so well to this.' has level 2.\n            A response containing the text 'really sorry going through this.' has level 2.", 'sp_interpretation_level': "The interpretation level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'People ask me why I'm zoned out most of the time. I'm not zoned out, I'm just in bot mode so I don't have to be attached to anything. It helps me supress the depressive thoughts but every night, it all just flows back in. I hate it.' has level 1.\n            A response containing the text 'i skipped one class today because i don't really get much out of the class, but i forgot that there was 5% of the grade for in-class activities, so i'll probably not skip any more classes' has level 1.\n            A response containing the text 'Actually I find myself taking much longer showers when I'm depressed. Sometimes twice in a day. It's the only time I feel relaxed.' has level 1.\n            A response containing the text 'No, that's what I'm doing and it isn't working.' has level 2.\n            A response containing the text 'I stopped catering to my problems... they just compile. I stopped obsessing over them or handling them in anyway. I have no control over my life. I simply look at my problems from my couch...' has level 2.\n            A response containing the text 'I understand how you feel.' has level 2.        \n            ", 'sp_explorations_level': "The explorations level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'What can we do today that will help?' has level 1.\n            A response containing the text 'What happened?' has level 1.\n            A response containing the text 'What makes you think you're a shitty human being?' has level 1.\n            A response containing the text 'What makes you say these things?' has level 2.\n            A response containing the text 'What do you feel is bringing on said troubling thoughts?' has level 2.\n            A response containing the text 'Do you have any friends that aren't always going to blow sunshine up your ass or a therapist?' has level 2.        \n            "}', '
        Given posts and responses from a social media forum, extract certain linguistic features on a scale from 0.0 to 2.0.        
        ')
	> Generated possible columns: [{'feature_name': 'sp_emotional_reaction_level', 'feature_prompt': "The emotional reaction level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'I'm with you all the way man.' has level 1.\n            A response containing the text 'I agree with him' has level 1.\n            A response containing the text 'Just keep practicing and never give up.' has level 1.\n            A response containing the text 'I'm so sorry you feel that way.' has level 2.\n            A response containing the text 'Holy shit I can relate so well to this.' has level 2.\n            A response containing the text 'really sorry going through this.' has level 2.", 'input_columns': ['post', 'response']}, {'feature_name': 'sp_interpretation_level', 'feature_prompt': "The interpretation level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'People ask me why I'm zoned out most of the time. I'm not zoned out, I'm just in bot mode so I don't have to be attached to anything. It helps me supress the depressive thoughts but every night, it all just flows back in. I hate it.' has level 1.\n            A response containing the text 'i skipped one class today because i don't really get much out of the class, but i forgot that there was 5% of the grade for in-class activities, so i'll probably not skip any more classes' has level 1.\n            A response containing the text 'Actually I find myself taking much longer showers when I'm depressed. Sometimes twice in a day. It's the only time I feel relaxed.' has level 1.\n            A response containing the text 'No, that's what I'm doing and it isn't working.' has level 2.\n            A response containing the text 'I stopped catering to my problems... they just compile. I stopped obsessing over them or handling them in anyway. I have no control over my life. I simply look at my problems from my couch...' has level 2.\n            A response containing the text 'I understand how you feel.' has level 2.        \n            ", 'input_columns': ['post', 'response']}, {'feature_name': 'sp_explorations_level', 'feature_prompt': "The explorations level of the response a scale from 0.0 to 2.0.\n            Most responses will have level 0.\n            A response containing the text 'What can we do today that will help?' has level 1.\n            A response containing the text 'What happened?' has level 1.\n            A response containing the text 'What makes you think you're a shitty human being?' has level 1.\n            A response containing the text 'What makes you say these things?' has level 2.\n            A response containing the text 'What do you feel is bringing on said troubling thoughts?' has level 2.\n            A response containing the text 'Do you have any friends that aren't always going to blow sunshine up your ass or a therapist?' has level 2.        \n            ", 'input_columns': ['post', 'response']}]
	> Querying 'openai/gpt-4.1' with 2 messages...'
Chosen GPU: 1
	> An error occurred in attempt 1: Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`
	> Querying 'openai/gpt-4.1' with 4 messages...'
Chosen GPU: 1
	> An error occurred in attempt 2: name 'end' is not defined
	> Querying 'openai/gpt-4.1' with 6 messages...'
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|██████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  3.92it/s]
Extracting sp_interpretation_level: 100%|██████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  3.62it/s]
Extracting sp_explorations_level: 100%|████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  4.46it/s]
	> Code executed successfully on a sample dataframe.
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|██████████████████████████████████████████████████████████████████| 189/189 [00:45<00:00,  4.20it/s]
Extracting sp_interpretation_level: 100%|██████████████████████████████████████████████████████████████████████| 189/189 [00:53<00:00,  3.50it/s]
Extracting sp_explorations_level: 100%|████████████████████████████████████████████████████████████████████████| 189/189 [00:47<00:00,  4.02it/s]
	> Generated columns: ['post', 'response', 'sp_emotional_reaction_level', 'sp_interpretation_level', 'sp_explorations_level']. 
 Code: import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm

def pick_gpu_by_free_mem_torch():
    assert torch.cuda.is_available(), "No CUDA device"
    free = []
    for i in range(torch.cuda.device_count()):
        # returns (free, total) in bytes for that device
        f, t = torch.cuda.mem_get_info(i)
        free.append((f, i))
    free.sort(reverse=True)
    _, idx = free[0]
    print(f"Chosen GPU: {idx}")
    return idx

gpu_idx = pick_gpu_by_free_mem_torch()
device = torch.device(f"cuda:{gpu_idx}")

MODEL_NAME = "google/flan-t5-large"  # Use a smaller model for compatibility

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
model = model.to(device)
model.eval()

def batchify(lst, batch_size):
    for i in range(0, len(lst), batch_size):
        yield lst[i:i+batch_size]

def extract_features(df: pd.DataFrame, features_to_extract: list[dict[str, object]]) -> pd.DataFrame:
    BATCH_SIZE = 8

    for feature in features_to_extract:
        feature_name = feature['feature_name']
        feature_prompt = feature['feature_prompt']
        input_columns = feature['input_columns']

        texts = []
        for idx, row in df.iterrows():
            post = row[input_columns[0]]
            response = row[input_columns[1]]
            prompt = (
                f"{feature_prompt.strip()}\n"
                f"Post: {post}\n"
                f"Response: {response}\n"
                f"Answer with a single number (0, 1, or 2) or a float between 0.0 and 2.0."
            )
            texts.append(prompt)

        results = []
        for batch in tqdm(list(batchify(texts, BATCH_SIZE)), desc=f"Extracting {feature_name}"):
            inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=8,
                    do_sample=False,
                    num_beams=1,
                )
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            for out in decoded:
                try:
                    val = float(out.strip().split()[0])
                except Exception:
                    val = 0.0
                results.append(val)

        df[feature_name] = results

    return df

/home/schelter-ldap/.cache/pypoetry/virtualenvs/sempipes-q6T4qXBB-py3.12/lib/python3.12/site-packages/interpret/glassbox/_ebm/_ebm.py:1250: UserWarning: For multiclass we cannot currently visualize pairs and they will be stripped from the global explanations. Set interactions=0 to generate a fully interpretable glassbox model.
  warn(
Chosen GPU: 1
Extracting sp_emotional_reaction_level: 100%|██████████████████████████████████████████████████████████████████| 189/189 [00:44<00:00,  4.21it/s]
Extracting sp_interpretation_level: 100%|██████████████████████████████████████████████████████████████████████| 189/189 [00:54<00:00,  3.49it/s]
Extracting sp_explorations_level: 100%|████████████████████████████████████████████████████████████████████████| 189/189 [00:46<00:00,  4.11it/s]
	> Generated columns: ['post', 'response', 'sp_emotional_reaction_level', 'sp_interpretation_level', 'sp_explorations_level']. 
 Code: import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm

def pick_gpu_by_free_mem_torch():
    assert torch.cuda.is_available(), "No CUDA device"
    free = []
    for i in range(torch.cuda.device_count()):
        # returns (free, total) in bytes for that device
        f, t = torch.cuda.mem_get_info(i)
        free.append((f, i))
    free.sort(reverse=True)
    _, idx = free[0]
    print(f"Chosen GPU: {idx}")
    return idx

gpu_idx = pick_gpu_by_free_mem_torch()
device = torch.device(f"cuda:{gpu_idx}")

MODEL_NAME = "google/flan-t5-large"  # Use a smaller model for compatibility

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
model = model.to(device)
model.eval()

def batchify(lst, batch_size):
    for i in range(0, len(lst), batch_size):
        yield lst[i:i+batch_size]

def extract_features(df: pd.DataFrame, features_to_extract: list[dict[str, object]]) -> pd.DataFrame:
    BATCH_SIZE = 8

    for feature in features_to_extract:
        feature_name = feature['feature_name']
        feature_prompt = feature['feature_prompt']
        input_columns = feature['input_columns']

        texts = []
        for idx, row in df.iterrows():
            post = row[input_columns[0]]
            response = row[input_columns[1]]
            prompt = (
                f"{feature_prompt.strip()}\n"
                f"Post: {post}\n"
                f"Response: {response}\n"
                f"Answer with a single number (0, 1, or 2) or a float between 0.0 and 2.0."
            )
            texts.append(prompt)

        results = []
        for batch in tqdm(list(batchify(texts, BATCH_SIZE)), desc=f"Extracting {feature_name}"):
            inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=8,
                    do_sample=False,
                    num_beams=1,
                )
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            for out in decoded:
                try:
                    val = float(out.strip().split()[0])
                except Exception:
                    val = 0.0
                results.append(val)

        df[feature_name] = results

    return df
F1 score on 4: 0.7314814814814815

Mean final score:  0.69021164021164 0.027536111769077828