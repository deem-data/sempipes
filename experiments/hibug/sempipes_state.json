{
    "generated_code": "import torch\nimport pandas as pd\nfrom transformers import pipeline\nfrom PIL import Image\nfrom tqdm.auto import tqdm # For progress bars\n\n# Provided device picking function\ndef pick_device():\n    # Prefer MPS for Apple Silicon, then CUDA, then CPU\n    if torch.backends.mps.is_available():\n        print(\"Using MPS device\")\n        return torch.device(\"mps\")\n    elif torch.cuda.is_available():\n        # Find GPU with most free memory\n        free = []\n        for i in range(torch.cuda.device_count()):\n            f, t = torch.cuda.mem_get_info(i)\n            free.append((f, i))\n        free.sort(reverse=True)\n        _, idx = free[0]\n        print(f\"Chosen GPU: {idx}\")\n        return torch.device(f\"cuda:{idx}\")\n    else:\n        print(\"Using CPU\")\n        return torch.device(\"cpu\")\n\ndevice = pick_device()\n\nfeatures_to_extract = [\n    # Feature 'beard': Yes/No. Relevant to lipstick as facial hair can obscure the mouth area.\n    {'feature_name': 'beard', 'feature_prompt': 'Does the person have a beard?', 'input_columns': ['image'], 'candidate_labels': ['yes', 'no']},\n    # Feature 'makeup': Yes/No. Directly correlated with lipstick as lipstick is a type of makeup.\n    {'feature_name': 'makeup', 'feature_prompt': 'Does the person wear makeup?', 'input_columns': ['image'], 'candidate_labels': ['yes', 'no']},\n    # Feature 'gender': Male/Female. Makeup wearing habits often vary significantly between genders.\n    {'feature_name': 'gender', 'feature_prompt': 'Is the person in the photo a male or a female?', 'input_columns': ['image'], 'candidate_labels': ['male', 'female']},\n    # Feature 'hair_color': Specific colors. Could be a confounding factor for lipstick detection due to contrast or visual attention shifts.\n    {'feature_name': 'hair_color', 'feature_prompt': 'Which of the following hair colors does the person in the photo have: blonde, brown, black, gray, white or red?', 'input_columns': ['image'], 'candidate_labels': ['blonde', 'brown', 'black', 'gray', 'white', 'red']},\n    # FEATURE CHANGE REASONING:\n    # 1. 'skin_color': Changed candidate labels from ['white', 'brown', 'black'] to ['light', 'medium', 'dark'].\n    #    Rationale: These terms are more focused on visual perception of skin tone rather than broad racial/ethnic categories.\n    #    A model attempting to classify lipstick might experience failures or variations in performance across a continuous\n    #    spectrum of skin tones, which 'light', 'medium', 'dark' might better capture compared to more distinct\n    #    'white', 'brown', 'black'. This might provide more nuanced features for debugging, particularly\n    #    if contrast between lipstick and skin is an issue. These labels also adhere to the \"single word\" constraint.\n    {'feature_name': 'skin_color', 'feature_prompt': 'Does the person in the photo have light, medium or dark skin?', 'input_columns': ['image'], 'candidate_labels': ['light', 'medium', 'dark']},\n    # FEATURE CHANGE REASONING:\n    # 2. 'emotion': Changed 'happy' in candidate labels to 'smiling'.\n    #    Rationale: \"Happy\" is an internal emotional state, whereas \"smiling\" is a direct facial expression\n    #    that significantly alters the shape and visibility of the lips. Variations in lip shape due to\n    #    smiling (e.g., lips stretching, revealing teeth) can be a significant failure mode for lipstick\n    #    detection algorithms. Explicitly detecting 'smiling' should provide a more precise attribute for debugging.\n    {'feature_name': 'emotion', 'feature_prompt': 'Which of the following emotions is the person in the photo showing: sad, serious, calm, smiling, surprised, neutral, angry, excited, pensive?', 'input_columns': ['image'], 'candidate_labels': ['sad', 'serious', 'calm', 'smiling', 'surprised', 'neutral', 'angry', 'excited', 'pensive']},\n    # Feature 'age': Young/Middle-aged/Old. Lip features (e.g., plumpness, lines) and makeup styles can vary by age, affecting lipstick detection.\n    {'feature_name': 'age', 'feature_prompt': 'Which of the following age ranges is the person in the photo in: young, middle-aged, old?', 'input_columns': ['image'], 'candidate_labels': ['young', 'middle-aged', 'old']}\n]\n\n# Helper function to convert device object to pipeline's 'device' parameter format\ndef get_pipeline_device(device_obj):\n    if device_obj.type == \"cuda\":\n        # For CUDA, `pipeline` expects an integer device ID (e.g., 0, 1)\n        return device_obj.index\n    elif device_obj.type == \"mps\":\n        # For MPS, `pipeline` expects the device object itself or string 'mps'\n        return device_obj\n    else: # cpu\n        # For CPU, `pipeline` expects -1\n        return -1\n\n# Batch size definition for processing images. Adjusted for potential VRAM usage with a large CLIP model.\nBATCH_SIZE = 16 # A smaller batch size for the `large-patch14` model is often safer to avoid OOM errors, especially on smaller GPUs or MPS.\n\ndef extract_features(df: pd.DataFrame) -> pd.DataFrame:\n    # Reasoning for changes in this block:\n    # The core logic for feature extraction remains robust from the previous iteration.\n    # The performance improvements for this step are targeted at the *quality* of the features\n    # being extracted, which is primarily influenced by the prompts and candidate labels.\n    # No changes are made to the `transformers` model choice (still `openai/clip-vit-large-patch14`)\n    # as it's a powerful and versatile model for zero-shot image classification.\n    # Device handling and batching are already optimized.\n    # The specific modifications are within the `features_to_extract` list,\n    # as detailed in the comments above this function definition.\n\n    # Ensure 'image' column exists and is not empty\n    if 'image' not in df.columns or df['image'].empty:\n        print(\"DataFrame does not contain an 'image' column or it is empty.\")\n        return df\n\n    # Get a list of all image paths to process\n    image_paths = df['image'].tolist()\n\n    # Initialize the zero-shot image classification pipeline once.\n    print(f\"Initializing zero-shot-image-classification pipeline with openai/clip-vit-large-patch14 on {device}...\")\n\n    # Configure the device for the Hugging Face pipeline\n    pipe_device = get_pipeline_device(device)\n\n    classifier = pipeline(\n        \"zero-shot-image-classification\",\n        model=\"openai/clip-vit-large-patch14\",\n        device=pipe_device,\n        batch_size=BATCH_SIZE\n    )\n\n    # Dictionary to store results for new columns\n    results = {feature['feature_name']: [] for feature in features_to_extract}\n\n    print(f\"Extracting {len(features_to_extract)} features from {len(df)} images in batches (batch size: {BATCH_SIZE})...\")\n\n    for i in tqdm(range(0, len(image_paths), BATCH_SIZE), desc=\"Processing image batches\"):\n        batch_image_paths = image_paths[i:i + BATCH_SIZE]\n        batch_images = [Image.open(path).convert(\"RGB\") for path in batch_image_paths]\n\n        for feature_config in features_to_extract:\n            feature_name = feature_config['feature_name']\n            candidate_labels = feature_config['candidate_labels']\n\n            predictions_batch = classifier(\n                image=batch_images,\n                candidate_labels=candidate_labels,\n            )\n\n            for pred_list in predictions_batch:\n                best_label = pred_list[0]['label']\n                results[feature_name].append(best_label)\n\n    for feature_name, feature_values in results.items():\n        df[feature_name] = feature_values\n\n    return df\n"
}
  