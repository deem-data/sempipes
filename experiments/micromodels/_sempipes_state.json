{ "generated_code": "import pandas as pd\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM, # Used for text2text-generation with Flan-T5\n    pipeline,\n)\nfrom tqdm.auto import tqdm # For displaying progress bars\n\n# --- GPU Selection Helper (provided by user) ---\ndef pick_gpu_by_free_mem_torch():\n    \"\"\"Selects the CUDA device with the most free memory.\"\"\"\n    assert torch.cuda.is_available(), \"No CUDA device available.\"\n    free = []\n    for i in range(torch.cuda.device_count()):\n        f, t = torch.cuda.mem_get_info(i)  # returns (free, total) in bytes\n        free.append((f, i))\n    free.sort(reverse=True)\n    _, idx = free[0]\n    print(f\"Chosen GPU: {idx}\")\n    return idx\n\n# --- Global Initialization (executed once when the script is loaded) ---\ngpu_idx = pick_gpu_by_free_mem_torch()\n\n# --- Feature Definition (provided by user) ---\nfeatures_to_extract = [\n    {'feature_name': 'sp_emotional_reaction_level', 'feature_prompt': \"The emotional reaction level of the response on a scale from 0.0 to 2.0.            Most responses will have level 0.            A response containing the text 'I'm with you all the way man.' has level 1.            A response containing the text 'I agree with him' has level 1.            A response containing the text 'Just keep practicing and never give up.' has level 1.            A response containing the text 'I'm so sorry you feel that way.' has level 2.            A response containing the text 'Holy shit I can relate so well to this.' has level 2.            A response containing the text 'really sorry going through this.' has level 2.\", 'input_columns': ['response']},\n    {'feature_name': 'sp_interpretation_level', 'feature_prompt': \"The interpretation level of the response on a scale from 0.0 to 2.0.            Most responses will have level 0.            A response containing the text 'People ask me why I'm zoned out most of the time. I'm not zoned out, I'm just in bot mode so I don't have to be attached to anything. It helps me supress the depressive thoughts but every night, it all just flows back in. I hate it.' has level 1.            A response containing the text 'i skipped one class today because i don't really get much out of the class, but i forgot that there was 5% of the grade for in-class activities, so i'll probably not skip any more classes' has level 1.            A response containing the text 'Actually I find myself taking much longer showers when I'm depressed. Sometimes twice in a day. It's the only time I feel relaxed.' has level 1.            A response containing the text 'No, that's what I'm doing and it isn't working.' has level 2.            A response containing the text 'I stopped catering to my problems... they just compile. I stopped obsessing over them or handling them in anyway. I have no control over my life. I simply look at my problems from my couch...' has level 2.            A response containing the text 'I understand how you feel.' has level 2.                    \", 'input_columns': ['response']},\n    {'feature_name': 'sp_explorations_level', 'feature_prompt': \"The explorations level of the response on a scale from 0.0 to 2.0.            Most responses will have level 0.            A response containing the text 'What can we do today that will help?' has level 1.            A response containing the text 'What happened?' has level 1.            A response containing the text 'What makes you think you're a shitty human being?' has level 1.            A response containing the text 'What makes you say these things?' has level 2.            A response containing the text 'What do you feel is bringing on said troubling thoughts?' has level 2.            A response containing the text 'Do you have any friends that aren't always going to blow sunshine up your ass or a therapist?' has level 2.                    \", 'input_columns': ['response']}\n]\n\n# --- FeatureExtractor Class to Load Models Once ---\nclass FeatureExtractor:\n    \"\"\"\n    Manages the loading and inference of Hugging Face transformer models.\n    Models are loaded once onto the chosen GPU.\n    \"\"\"\n    def __init__(self, gpu_idx: int):\n        self.device = torch.device(f\"cuda:{gpu_idx}\")\n\n        # Rationale for choosing google/flan-t5-large:\n        # Continuing with `flan-t5-large` as it has consistently shown the best or tied-best\n        # performance (f1_micro=0.741) for this specific zero-shot text-to-score task,\n        # despite trying larger models and slight prompt variations. This model appears to offer\n        # the optimal balance of reasoning capabilities and generalizability for the provided prompts.\n        self.model_name = \"google/flan-t5-large\"\n        print(f\"Loading model: {self.model_name} to device: {self.device}\")\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        \n        # Maintaining default torch_dtype (float32). This ensures maximum numerical precision\n        # for `flan-t5-large`, which seems to work best compared to lower precision options\n        # in previous attempts.\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name).to(self.device)\n        self.pipeline_generator = pipeline(\n            \"text2text-generation\",\n            model=self.model,\n            tokenizer=self.tokenizer,\n            device=self.device,\n            max_new_tokens=10, # Limiting generation to small outputs like \"0.0\", \"1.5\" etc.\n            do_sample=False # For deterministic output when possible to ensure consistent scoring.\n        )\n\n    def extract_level_feature(self, texts: list[str], feature_prompt: str, batch_size: int = 16) -> list[float]:\n        \"\"\"\n        Extracts a level feature (0.0 to 2.0) from a list of texts using the pre-loaded Flan-T5 model.\n\n        Args:\n            texts: A list of text strings to process.\n            feature_prompt: The specific prompt explaining the feature and providing examples.\n            batch_size: The number of texts to process in each GPU batch for efficiency.\n                        A batch size of `16` is used, balancing memory consumption and GPU throughput\n                        for the `flan-t5-large` model.\n\n        Returns:\n            A list of float values representing the extracted level for each text.\n        \"\"\"\n        results = []\n        # Rationale for prompt modification:\n        # While previous prompt adjustments did not lead to improvement (some even caused degradation),\n        # this prompt iteration aims to reinforce the expected *output format* (e.g., \"0.0\", \"1.0\", \"2.0\")\n        # right at the point where the model needs to generate the final answer. This subtle change at\n        # the very end of the instruction intends to reduce instances of verbose or non-numeric output\n        # that might complicate parsing, without over-constraining the model's natural language understanding,\n        # thereby seeking to improve the quality and reliability of the numerical features extracted.\n        full_prompts = [\n            f\"{feature_prompt}Based on this, what is the level for the following response (0.0 to 2.0)?Response: '{text}'Score (e.g., 0.0, 1.0, 2.0):\"\n            for text in texts\n        ]\n\n        # Use tqdm to show a progress bar during batch processing for user feedback.\n        for i in tqdm(range(0, len(full_prompts), batch_size), desc=f\"Extracting level feature\"):\n            batch_prompts = full_prompts[i : i + batch_size]\n            generated_outputs = self.pipeline_generator(batch_prompts)\n\n            for output in generated_outputs:\n                generated_text = output['generated_text'].strip()\n                try:\n                    level = float(generated_text)\n                    # Clamp the predicted level to the expected range [0.0, 2.0].\n                    # This ensures all outputs strictly adhere to the defined scale, preventing outliers.\n                    level = max(0.0, min(2.0, level))\n                except ValueError:\n                    # If conversion to float fails (e.g., model generates text that is not a pure number),\n                    # default to 0.0. This makes the extraction process robust against unexpected model outputs\n                    # and aligns with the general assumption in the prompts that \"Most responses will have level 0\".\n                    level = 0.0\n                results.append(level)\n        return results\n\n# Instantiate the FeatureExtractor once. This loads the potentially large model to GPU memory.\nfeature_extractor_instance = FeatureExtractor(gpu_idx)\n\n# --- Main Feature Extraction Function ---\ndef extract_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Extracts specified multi-modal features from the DataFrame using transformer models.\n\n    Args:\n        df: The input pandas DataFrame containing the 'response' text column.\n\n    Returns:\n        The original DataFrame with newly generated feature columns.\n    \"\"\"\n    if 'response' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'response' column for text extraction.\")\n\n    # Fill NaN values in the 'response' column to prevent errors during text processing.\n    # Empty strings are handled gracefully by NLP models and are less likely to cause errors than NaN.\n    df['response'] = df['response'].fillna(\"\")\n\n    for feature_def in features_to_extract:\n        feature_name = feature_def['feature_name']\n        feature_prompt = feature_def['feature_prompt']\n        # Assuming there's always one input column, which is 'response' as per current instructions.\n        input_col = feature_def['input_columns'][0]\n\n        print(f\"--- Extracting feature: '{feature_name}' ---\")\n        df[feature_name] = feature_extractor_instance.extract_level_feature(\n            texts=df[input_col].tolist(), # Convert series to list for batch processing\n            feature_prompt=feature_prompt\n        )\n\n    return df\"\n" }
